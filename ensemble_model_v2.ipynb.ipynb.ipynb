{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oc8ntumSdNL-"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjoblib\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparse\u001b[39;00m \u001b[39mimport\u001b[39;00m urlparse\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from urllib.parse import urlparse\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import tldextract\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import requests\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "esIc5i2rddZM"
      },
      "outputs": [],
      "source": [
        "def parse_url(url: str):\n",
        "    try:\n",
        "        no_scheme = not url.startswith('https://') and not url.startswith('http://')\n",
        "        if no_scheme:\n",
        "            parsed_url = urlparse(f\"http://{url}\")\n",
        "            return {\n",
        "                \"url\": url,\n",
        "                \"scheme\": None,\n",
        "                \"netloc\": parsed_url.netloc,\n",
        "                \"path\": parsed_url.path,\n",
        "                \"params\": parsed_url.params,\n",
        "                \"query\": parsed_url.query,\n",
        "                \"fragment\": parsed_url.fragment,\n",
        "            }\n",
        "        else:\n",
        "            parsed_url = urlparse(url)\n",
        "            return {\n",
        "                \"url\": url,\n",
        "                \"scheme\": parsed_url.scheme,\n",
        "                \"netloc\": parsed_url.netloc,\n",
        "                \"path\": parsed_url.path,\n",
        "                \"params\": parsed_url.params,\n",
        "                \"query\": parsed_url.query,\n",
        "                \"fragment\": parsed_url.fragment,\n",
        "            }\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fqnM-YWwdgad"
      },
      "outputs": [],
      "source": [
        "def get_num_subdomains(netloc: str):\n",
        "    subdomain = tldextract.extract(netloc).subdomain\n",
        "    if subdomain == \"\":\n",
        "        return 0\n",
        "    return subdomain.count('.') + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iOnnPR5ydi6c"
      },
      "outputs": [],
      "source": [
        "def tokenize_domain(netloc: str):\n",
        "    tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
        "    split_domain = tldextract.extract(netloc)\n",
        "    no_tld = str(split_domain.subdomain +'.'+ split_domain.domain)\n",
        "    return \" \".join(map(str,tokenizer.tokenize(no_tld)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8Jb_YFb-dl1Q"
      },
      "outputs": [],
      "source": [
        "def predict_url(model, url: str):\n",
        "    parsed_url = parse_url(url)\n",
        "    data = pd.DataFrame.from_records([parsed_url])\n",
        "    data[\"length\"] = data.url.str.len()\n",
        "    data[\"tld\"] = data.netloc.apply(lambda nl: tldextract.extract(nl).suffix)\n",
        "    data['tld'] = data['tld'].replace('','None')\n",
        "    data[\"is_ip\"] = data.netloc.str.fullmatch(r\"\\d+\\.\\d+\\.\\d+\\.\\d+\")\n",
        "    data['domain_hyphens'] = data.netloc.str.count('-')\n",
        "    data['domain_underscores'] = data.netloc.str.count('_')\n",
        "    data['path_hyphens'] = data.path.str.count('-')\n",
        "    data['path_underscores'] = data.path.str.count('_')\n",
        "    data['slashes'] = data.path.str.count('/')\n",
        "    data['full_stops'] = data.path.str.count('.')\n",
        "    data['num_subdomains'] = data['netloc'].apply(lambda net: get_num_subdomains(net))\n",
        "    data['domain_tokens'] = data['netloc'].apply(lambda net: tokenize_domain(net))\n",
        "    data['path_tokens'] = data['path'].apply(lambda path: \" \".join(map(str,tokenizer.tokenize(path))))\n",
        "    data.drop(['url', 'scheme', 'netloc', 'path', 'params', 'query', 'fragment'], axis=1, inplace=True)\n",
        "\n",
        "    pred = model.predict(data)\n",
        "    pred_proba = model.predict_proba(data)\n",
        "\n",
        "    return pred[0], max(pred_proba[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tYSE_p1ZeKWd"
      },
      "outputs": [],
      "source": [
        "class Converter(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, data_frame):\n",
        "        return data_frame.values.ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n9e0W2HBeewG"
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer(r'[A-Za-z]+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LqGj5BVodoWw"
      },
      "outputs": [],
      "source": [
        "clf = joblib.load('naive_bayes_classifier.pkl')\n",
        "preprocessor = joblib.load('preprocessor.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J119_iSYfxai"
      },
      "outputs": [],
      "source": [
        "API_URL = \"https://api-inference.huggingface.co/models/Jagannath/phishNet\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2li5aTE4seAg"
      },
      "outputs": [],
      "source": [
        "def extract_urls(text):\n",
        "    # Regular expression pattern to find URLs starting with 'www.' or 'https://'\n",
        "    url_pattern = r'\\b(https?://[^\\s]+|www\\.[^\\s]+)'\n",
        "\n",
        "    # Find all URLs in the text using the pattern\n",
        "    urls = re.findall(url_pattern, text)\n",
        "\n",
        "    return urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Wzz1EvolqttX"
      },
      "outputs": [],
      "source": [
        "def get_predictions(url, text):\n",
        "    url_prediction, url_confidence = predict_url(clf, url)\n",
        "\n",
        "    url_list = extract_urls(text)\n",
        "    goodCount = 0\n",
        "    badCount = 0\n",
        "    for url in url_list:\n",
        "      prediction, confidence = predict_url(clf, url)\n",
        "      if prediction == 'good':\n",
        "        goodCount += 1\n",
        "      else:\n",
        "        badCount += 1\n",
        "    url_pred = None\n",
        "    if goodCount > badCount:\n",
        "      url_pred = 'good'\n",
        "    elif badCount > goodCount:\n",
        "      url_pred = 'bad'\n",
        "    elif goodCount == badCount == 0:\n",
        "      url_pred = None\n",
        "\n",
        "    if url_pred != None:\n",
        "      url_prediction = url_pred\n",
        "\n",
        "    # Text prediction\n",
        "    output = query({\n",
        "    \"inputs\": text,\n",
        "    \"parameters\": {\n",
        "      \"truncation\": \"only_first\"\n",
        "    }\n",
        "    })\n",
        "\n",
        "    max_confidence_label = max(output[0], key=lambda x: x['score'])\n",
        "    predicted_label = \"good\" if max_confidence_label['label'] == 'LABEL_0' else \"bad\"\n",
        "    text_confidence_score = max_confidence_label['score']\n",
        "\n",
        "    return url_prediction, url_confidence, predicted_label, text_confidence_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6QXZXcpBq8Y6"
      },
      "outputs": [],
      "source": [
        "text = '''Commbank\n",
        "     Dear Commonwealth Bank Client\n",
        "      We are contacting you to inform you that on October 7, 2006 our Account\n",
        "   Review Team identified some usual activity in your account. In accordance\n",
        "   with User Agreement and to ensure that your account has not been\n",
        "   compromised, access to your account was limited. Your account access will\n",
        "   remain limited until this issue has been resolved.\n",
        "   We encourage you to log in and perform the steps  necessary to restore your\n",
        "   account access as soon as possible. Allowing your account access to remain\n",
        "   limited for an extended period of time may result in further limitations on\n",
        "   the use of your account and possible account closure.\n",
        "   In order to confirm your account and to preserve the account stability, you\n",
        "   are required to login to your account using the following link below:\n",
        "   [1]http://www.commonwealth-updatesystem.com\n",
        "   This procedure is performed one time only and it does not require further\n",
        "   actions on the customer side. This is an automated message, no reply or\n",
        "   confirmation is required. Thank you for using Commonwealth NetBank!\n",
        "        Ã‚Â© Copyright Commonwealth Bank of Australia 2005 ABN 48 123 123 124\n",
        "\n",
        "References\n",
        "\n",
        "   1. http://www.commonwealth-updatesystem.com/'''\n",
        "url_test = \"http://www.commonwealth-updatesystem.com/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya4tV8Oaq_LG",
        "outputId": "9d53856d-6841-4158-a787-2e7b748b3a43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('good', 0.682154050869551, 'bad', 0.9621559977531433)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_predictions(url_test, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Hw1malN_sKGD"
      },
      "outputs": [],
      "source": [
        "def ensemble_prediction(url, text):\n",
        "    url_prediction, url_confidence, text_prediction, text_confidence = get_predictions(url, text)\n",
        "    if url_prediction == \"None\":\n",
        "      print(url_prediction + \" : \" + str(url_confidence))\n",
        "    if text_prediction == \"None\":\n",
        "      print(text_prediction + \" : \" + str(text_confidence))\n",
        "    if url_prediction == text_prediction:\n",
        "        print(url_prediction + \" : \" + str((url_confidence + text_confidence)/2))\n",
        "    else:\n",
        "      if url_confidence > text_confidence:\n",
        "        print(url_prediction + \" : \" + str((url_confidence + text_confidence)/2))\n",
        "      else:\n",
        "        print(text_prediction + \" : \" + str((url_confidence + text_confidence)/2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzvSv1DGsMvU",
        "outputId": "7161df28-c865-4545-e98d-847f36476830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bad : 0.8221550243113471\n"
          ]
        }
      ],
      "source": [
        "ensemble_prediction(url_test,text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
